{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day9_regressions.ipynb","provenance":[{"file_id":"1JRSZYQ3YPOFgwGF9TBjmUhdMRf4gbeIc","timestamp":1598370159747}],"private_outputs":true,"collapsed_sections":["PCdVU4bJJD2n","ch_2qbegzAbT","hWQtC_Xh8a5N","2zT7fhaS8a5f","-F0EF9sI8a5O","8FNmC37s8a5U","sc6NwGoF8a5V","2c3gtypuycwl","oFNhDcEbytBf","NDE5Gg_v8a5g","6oig_VhNMn4U","DIKUVS-O8a5h","KuTAuuX98a5k","9HGzOQeB8a5o","6rQmU9QK8a5q","05irb5yC8a5t","vZW6CrAe8a50","7fddGbwTHAmt","3i1V52UBVk_h","Maaa6-w-IuiS","dMWYcpdOJFvp","dYZ8h0wmXx_4","2ZVfwxXhXyAl","SM3VG230XyAn","neEnhUc2XbT4","rf1T_8rjXx_6","aB3KwxhmXx_6","pmNasS4mXyAN","5E53zlBV1TYf","gT98CLWnTRUW","DXGFbsYvXyAT","qr-CZgy6XyAr","2MivgaVBXyBE","6IG4PB18XyBS","3MIfhcVGXyBW","KIp0Za4XXyBp","LUpw7sWHR1Zq","jXtvZnBlXMLl","qQHaN2OaWzw-","J4mvEl5WYKOq","V0Fo03ABbbOK","gyVPiDoVch-9","NvX3vHTxgIlu","ctwzD3xvhWQd","7rPrEfawZLkA","ejY_UnnRIhMv","x4LZ7Gb1Mn5E","mk0uR5s13wh2","F_PZq18JMn4u","5e0sO7DeMn4y","g5qyTwZSMn43","J9nIxsodMn47","wFxRPHqm8a6B","q4ixiWWj8a6I"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PCdVU4bJJD2n","colab_type":"text"},"source":["# Coding II -- Day 9 -- Regressions in research\n","-- Haynes Stephens -- haynes13@uchicago.edu"]},{"cell_type":"markdown","metadata":{"id":"5Kaq89ia22KG","colab_type":"text"},"source":["**NOTE**: If you're following along to a live lecture, don't worry about reading the text in this notebook. I'll summarize the text as we move forward, and it's there in case you need to go back and refresh your memory at a later time."]},{"cell_type":"markdown","metadata":{"id":"jYV826cLefT-","colab_type":"text"},"source":["Today we will be working on creating regression models in Python and seeing how we can take advantage of using `R` and `Python` together. While this lesson will cover topics in statistics and machine learning, the purpose of the lesson is to show you the tools available in `Python`. I'm no expert on the statistics behind these methods and may not be able to answer a lot of your theoretical questions, but the bootcamp session next week will be more tailored towards the theory. "]},{"cell_type":"markdown","metadata":{"id":"ch_2qbegzAbT","colab_type":"text"},"source":["## 0.1 Import required packages\n","\n","Let's import some of the stuff we'll need off the bat."]},{"cell_type":"code","metadata":{"id":"3GRfxPt1_xhF","colab_type":"code","colab":{}},"source":["# Data analysis\n","import numpy as np\n","import pandas as pd\n","%load_ext google.colab.data_table\n","\n","# Plotting\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import plotly.graph_objs as go\n","import plotly.express as px"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHKwxI83umEE","colab_type":"text"},"source":["Let's mount our drive so that we can have access to our Google Drive folders and files."]},{"cell_type":"code","metadata":{"id":"TBlUH7DaaKPa","colab_type":"code","colab":{}},"source":["# mount your google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","### List the directories you'll use\n","loaddir = '/content/drive/Shared drives/Coding_Bootcamps_2020/computing_for_research/data/'\n","savedir = '/content/drive/My Drive/my_bootcamp_2020/'     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWQtC_Xh8a5N","colab_type":"text"},"source":["# Section 1: Introducing Scikit-Learn"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2zT7fhaS8a5f","colab_type":"text"},"source":["## 1.1 Scikit-Learn's Estimator API"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"eIaJjGWQ8a5f","colab_type":"text"},"source":["The Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the [Scikit-Learn API paper](http://arxiv.org/abs/1309.0238):\n","\n","- *Consistency*: All objects share a common interface drawn from a limited set of methods, with consistent documentation.\n","\n","- *Inspection*: All specified parameter values are exposed as public attributes.\n","\n","- *Limited object hierarchy*: Only algorithms are represented by Python classes; datasets are represented\n","  in standard formats (NumPy arrays, Pandas ``DataFrame``s, SciPy sparse matrices) and parameter\n","  names use standard Python strings.\n","\n","- *Composition*: Many machine learning tasks can be expressed as sequences of more fundamental algorithms,\n","  and Scikit-Learn makes use of this wherever possible.\n","\n","- *Sensible defaults*: When models require user-specified parameters, the library defines an appropriate default value.\n","\n","In practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood.\n","Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Vr8UuQCu8a5N","colab_type":"text"},"source":["There are several Python libraries which provide solid implementations of a range of machine learning algorithms.\n","One of the best known is [Scikit-Learn](http://scikit-learn.org), a package that provides efficient versions of a large number of common algorithms.\n","Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation.\n","A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.\n","\n","This section provides an overview of the Scikit-Learn API; a solid understanding of these API elements will form the foundation for understanding the practical methods of machine learning.\n","\n","We will start by covering *data representation* in Scikit-Learn, followed by covering the *Estimator* API, and finally go through a more interesting example of using these tools for exploring a set of images of hand-written digits."]},{"cell_type":"code","metadata":{"id":"655CFr4st0aP","colab_type":"code","colab":{}},"source":["import sklearn\n","# we will load individual sklearn models and functions as we go along"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"-F0EF9sI8a5O","colab_type":"text"},"source":["## 1.2 Data Representation in Scikit-Learn"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SSEUdw4t8a5O","colab_type":"text"},"source":["Machine learning is all about creating regression models from data: for that reason, we'll start by discussing how data can be represented in order to be understood by the computer.\n","The best way to think about data within `scikit-learn` is in terms of tabular data."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Z40RbBcZ8a5P","colab_type":"text"},"source":["\n","A basic table is a two-dimensional grid of data, in which the rows represent individual elements of the dataset, and the columns represent values related to each of these elements.\n","For example, here is a dataset I made up about how much water a person needs to drink depending on the average daily temperature."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"MLx5VJGO8a5Q","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(42) \n","x = 70 * rng.rand(50) + 50\n","y = 0.25 * x - (rng.randn(50)*2) - 5\n","df = pd.DataFrame({\"temperature\":x, \"cups_of_water\":y})\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"RezcZZoW8a5T","colab_type":"text"},"source":["Each row of the dataset refers to a single observation, and the number of rows is the total number of observations in the dataset.\n","In general, we will refer to the rows of the matrix as *samples*, and the number of rows as ``n_samples``.\n","\n","Likewise, each column of the data refers to a particular quantitative piece of information that describes each sample.\n","In general, we will refer to the columns of the matrix as *features*, and the number of columns as ``n_features``. \n","\n","Here's a quick diagram to help show these structures."]},{"cell_type":"code","metadata":{"id":"MmDo8ivvBT_p","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(6, 4))\n","ax = fig.add_axes([0, 0, 1, 1])\n","ax.axis('off')\n","ax.axis('equal')\n","\n","color = 'black' # change to white if you ARE in DARK MODE\n","# Draw features matrix\n","ax.vlines(range(6), ymin=0, ymax=9, lw=1, color=color)\n","ax.hlines(range(10), xmin=0, xmax=5, lw=1, color=color)\n","font_prop = dict(size=12, family='monospace', color=color)\n","ax.text(-1, -1, \"Feature Matrix ($X$)\", size=14, color=color)\n","ax.text(0.1, -0.3, r'n_features $\\longrightarrow$', **font_prop)\n","ax.text(-0.1, 0.1, r'$\\longleftarrow$ n_samples', rotation=90,\n","        va='top', ha='right', **font_prop)\n","\n","# Draw labels vector\n","ax.vlines(range(8, 10), ymin=0, ymax=9, lw=1, color=color)\n","ax.hlines(range(10), xmin=8, xmax=9, lw=1, color=color)\n","ax.text(7, -1, \"Target Vector ($y$)\", size=14, color=color)\n","ax.text(7.9, 0.1, r'$\\longleftarrow$ n_samples', rotation=90,\n","        va='top', ha='right', **font_prop,)\n","\n","ax.set_ylim(10, -2)\n","plt.savefig(savedir+'regress_matricies.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"8FNmC37s8a5U","colab_type":"text"},"source":["### 1.2.1 Features matrix\n","\n","This table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will call the *features matrix*.\n","By convention, this features matrix is often stored in a variable named ``X``.\n","The features matrix is assumed to be two-dimensional, with shape ``[n_samples, n_features]``, and is most often contained in a `numpy` array or a `pandas` ``DataFrame``.\n","\n","The samples (i.e., rows) always refer to the individual objects described by the dataset.\n","For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\n","\n","The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner.\n","Features are generally real-valued, but may be Boolean or discrete-valued in some cases."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"sc6NwGoF8a5V","colab_type":"text"},"source":["### 1.2.2 Target array\n","\n","In addition to the feature matrix ``X``, we also generally work with a *label* or *target* array, which by convention we will usually call ``y``.\n","The target array is usually one dimensional, with length ``n_samples``, and is generally contained in a NumPy array or Pandas ``Series``.\n","The target array may have continuous numerical values, or discrete classes/labels.\n","We will be working with the common case of a one-dimensional target array.\n","\n","A common point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to *predict from the data*: in statistical terms, it is the dependent variable.\n","For example, in the preceding data we may wish to construct a model that can predict the cups of water that should be drank based on the daily temperature; in this case, the ``cups_of_water`` column would be considered the target array.\n","\n","With this target array in mind, we can use Plotly to conveniently visualize the data:"]},{"cell_type":"markdown","metadata":{"id":"2c3gtypuycwl","colab_type":"text"},"source":["## 1.3 Check-in 1\n","\n","Let's visualize this data real quick to see how it looks. Use either `seaborn.relplot()` or `plotly_express.scatter()` to make a plot of `cups of water` vs. `temperature`. Feel free to check out the **Visualization** notebook to refresh your knowledge of plot commands."]},{"cell_type":"code","metadata":{"id":"2zDySvwmypXw","colab_type":"code","colab":{}},"source":["# WRITE YOUR CODE IN THIS CELL"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oFNhDcEbytBf","colab_type":"text"},"source":["### Answer"]},{"cell_type":"code","metadata":{"id":"VIWDokiQH0RW","colab_type":"code","colab":{}},"source":["fig = px.scatter(\n","    df, \n","    x=\"temperature\",\n","    y = \"cups_of_water\",\n","    width = 800,\n","    height = 400,\n","    template = 'plotly_dark',\n","    )\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NDE5Gg_v8a5g","colab_type":"text"},"source":["## 1.4 Basics of the Scikit's API\n","\n","Most commonly, the steps in using the Scikit-Learn estimator API are as follows:\n","\n","1. Choose a class of model.\n","2. Choose model hyperparameters.\n","3. Arrange data into a features matrix and target vector.\n","4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n","5. Apply the model to new data using the ``predict()`` method.\n","\n","We will now step through an example of applying these steps."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"6oig_VhNMn4U","colab_type":"text"},"source":["# Section 2: Linear Regression\n","\n","We will start linear regression, where we assume that the target is related to the features by linear coefficients. The most familiar linear regression is a straight-line fit to data.\n","A straight-line fit is a model of the form\n","$$\n","y = wx + b\n","$$\n","where $w$ is commonly known as the *weight* (or slope), and $b$ is commonly known as the *bias* (or intercept)."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"DIKUVS-O8a5h","colab_type":"text"},"source":["## 2.1 Example\n","\n","As an example of this process, let's consider a simple linear regression—that is, the common case of fitting a line to $(x, y)$ data.\n","\n","Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"colab_type":"code","id":"-jTEJYOs4hSt","colab":{}},"source":["rng = np.random.RandomState(1)\n","x = 10 * rng.rand(50)\n","y = 2 * x - 5 + rng.randn(50)\n","plt.scatter(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KuTAuuX98a5k","colab_type":"text"},"source":["### 2.1.1 Choose a class of model\n","\n","In Scikit-Learn, every class of model is represented by a Python class.\n","So, for example, if we would like to compute a simple linear regression model, we can import the linear regression class:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"17t_WC9I8a5l","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"62II1j5x8a5n","colab_type":"text"},"source":["Note that other more general linear regression models exist as well; you can read more about them in the [``sklearn.linear_model`` module documentation](http://Scikit-Learn.org/stable/modules/linear_model.html)."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"9HGzOQeB8a5o","colab_type":"text"},"source":["### 2.1.2 Choose model hyperparameters\n","\n","An important point is that once we have decided on our model **class**, there are still some options open to as to the **instance** of model we want to use.\n","Depending on the model class we are working with, we might need to answer one or more questions like the following:\n","\n","- Would we like to fit for the offset (i.e., *y*-intercept)?\n","- Would we like the model to be normalized?\n","- Would we like to preprocess our data to add model flexibility?\n","- Would we like to prescribe regularization to use in our model?\n","- How many model components would we like to use?\n","\n","These are examples of the important choices that must be made **once the model class is selected**.\n","These choices are often represented as **hyperparameters**, or parameters that must be set before the model is fit to data.\n","In Scikit-Learn, **hyperparameters** are chosen by passing values at model instantiation.\n","\n","For our linear regression example, we can instantiate the `LinearRegression` class and specify that we would like to fit the intercept using the `fit_intercept` hyperparameter:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"NUT0cL3p8a5o","colab_type":"code","colab":{}},"source":["model = LinearRegression(fit_intercept=True)\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"uAPEEk-_8a5q","colab_type":"text"},"source":["Keep in mind that when the model is instantiated, the only action is the storing of these hyperparameter values.\n","In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between **choosing the model** and **applying the model to data**."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"6rQmU9QK8a5q","colab_type":"text"},"source":["### 2.1.3 Arrange data into a features matrix and target vector\n","\n","Previously we detailed the Scikit-Learn data representation, which requires a two-dimensional features matrix and a one-dimensional target array.\n","Here our target variable ``y`` is already in the correct form (a length-``n_samples`` array), but we need to massage the data ``x`` to make it a matrix of size ``[n_samples, n_features]``.\n","In this case, this amounts to a simple reshaping of the one-dimensional array:"]},{"cell_type":"code","metadata":{"id":"csdingdCJOff","colab_type":"code","colab":{}},"source":["x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"8CVqLxO18a5r","colab_type":"code","colab":{}},"source":["X = x[:, np.newaxis]\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"05irb5yC8a5t","colab_type":"text"},"source":["### 2.1.4 Fit the model to your data\n","\n","Now it is time to apply our model to data.\n","This can be done with the ``fit()`` method of the model:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"FXKJLcdy8a5t","colab_type":"code","colab":{}},"source":["model.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"dnyJ-zL58a5v","colab_type":"text"},"source":["This ``fit()`` command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in attributes that the user can explore.\n","In Scikit-Learn, by convention all model parameters that were learned during the `fit()` process have trailing underscores. For example in this linear model, the slope and intercept of the model fit are referred to as the `coef_` array and the `intercept_` value:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"RdWxkh_a8a5v","colab_type":"code","colab":{}},"source":["model.coef_, model.intercept_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7KXBoXTe8a50","colab_type":"text"},"source":["These two parameters represent the slope and intercept of the simple linear fit to the data.\n","\n","**Comparing to the data, we see that they are very close to the defined slope of 2 and intercept of -5.**\n","\n","One question that frequently comes up regards the uncertainty in such internal model parameters.\n","In general, Scikit-Learn does not provide tools to draw conclusions from internal model parameters themselves: interpreting model parameters is much more a **statistical modeling** question than a **machine learning** question.\n","Machine learning rather focuses on what the model *predicts*.\n","If you would like to dive into the meaning of fit parameters within the model, other tools are available, including the [Statsmodels Python package](http://statsmodels.sourceforge.net/). "]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"vZW6CrAe8a50","colab_type":"text"},"source":["### 2.1.5 Predict labels for unknown data\n","\n","Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was **not** part of the training set.\n","In Scikit-Learn, this can be done using the ``predict()`` method.\n","For the sake of this example, our \"new data\" will be a grid of *x* values, and we will ask what *y* values the model predicts:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"kHcFt8Ks8a51","colab_type":"code","colab":{}},"source":["xfit = np.linspace(-1, 11)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"1uWMZFsz8a54","colab_type":"text"},"source":["As before, we need to coerce these *x* values into a ``[n_samples, n_features]`` features matrix, after which we can feed it to the model:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"q9Ln90m58a56","colab_type":"code","colab":{}},"source":["Xfit = xfit[:, np.newaxis]\n","yfit = model.predict(Xfit)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"5epGclNB8a5-","colab_type":"text"},"source":["Finally, let's visualize the results by plotting first the raw data, and then this model fit:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"PguE0bZh8a5_","colab_type":"code","colab":{}},"source":["plt.scatter(x, y)       # A scatter of the data points\n","plt.plot(xfit, yfit)    # The line our model predicts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fddGbwTHAmt","colab_type":"text"},"source":["## 2.2 Check-in 2\n","\n","Going back to our **temperature-water** dataset:\n","1. Create a linear regression using Sci-KitLearn, where you fit *with* an intercept. Remember to massage the `x` data.\n","2. Plot the linear fit as a dashed line.\n","3. Print the slope and intercept values."]},{"cell_type":"code","metadata":{"id":"9klq0Wx87r4L","colab_type":"code","colab":{}},"source":["# WRITE YOUR CODE IN THIS CELL"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3i1V52UBVk_h","colab_type":"text"},"source":["### 2.2.1 Answer"]},{"cell_type":"code","metadata":{"id":"hd61qPjIVmN6","colab_type":"code","colab":{}},"source":["model = LinearRegression(fit_intercept=True)\n","\n","x = df[\"temperature\"]\n","x = x[:, np.newaxis]\n","y = df[\"cups_of_water\"]\n","\n","#rearange x to be a 2D vertial array with np.newaxis (y array is 1D)\n","model.fit(x, y) \n","\n","xfit = np.linspace(50, 120, 1000)\n","xfit = xfit[:, np.newaxis]\n","yfit = model.predict(xfit)\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, yfit, linestyle='dashed')\n","\n","print(\"Model slope:    \", model.coef_[0])\n","print(\"Model intercept:\", model.intercept_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Maaa6-w-IuiS","colab_type":"text"},"source":["## 2.3 EXTRA - Multiple regressors"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"colab_type":"text","id":"HZ-bCX6NOBFJ"},"source":["The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form\n","$$\n","y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n","$$\n","where there are multiple $x$ values.\n","Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions."]},{"cell_type":"code","metadata":{"id":"pmQQiT3Es_0t","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import PolynomialFeatures"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OanTA5sI0_Ft","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(1)\n","X = 10 * rng.rand(100, 3)\n","poly = PolynomialFeatures(3, include_bias=False)\n","XN = poly.fit_transform(X)\n","XN.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"-5ERTpvDMn4g","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(1)\n","X = 10 * rng.rand(100, 3)\n","y = 0.5 + np.dot(X, [1.5, -2., 1.])\n","\n","model.fit(X, y)\n","print(model.intercept_)\n","print(model.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5Q7PKllI9Zd","colab_type":"code","colab":{}},"source":["df = pd.DataFrame({\"X1\":X[:, 0],\n","                  \"X2\":X[:, 1],\n","                  \"X3\":X[:, 2],\n","                  \"Y\":y})\n","\n","fig = px.scatter_matrix(\n","    df, \n","    dimensions=[\"X1\", \"X2\", \"X3\", \"Y\"], \n","    # color=\"species\",\n","    width = 800,\n","    height = 400,\n","    opacity = 0.6,\n","    template = 'plotly_dark',\n","    )\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"eWpO3W5hMn4i","colab_type":"text"},"source":["Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.\n","\n","**Note**: The y variable shows a relationship with each of the X variables, and also important is that the X variables show no correlation to each other, a good sign. If our X variables were related then our model is probably hiding some influence from compounding variables.\n","\n","In this way, we can use the single ``LinearRegression`` estimator to fit lines, planes, or hyperplanes to our data.\n","It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well."]},{"cell_type":"code","metadata":{"id":"MwvANV3c5-7D","colab_type":"code","colab":{}},"source":["fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n","xfit = np.linspace(0, 10, 1000)\n","for i in range(len(axes)):\n","    ax = axes[i]\n","    ax.scatter(X[:, i], y)\n","    ax.plot(xfit, (xfit * model.coef_[i]) + model.intercept_)\n","    ax.set_title('$x_{0}$'.format(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMWYcpdOJFvp","colab_type":"text"},"source":["## 2.4 Basis Functions"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"zSGE1BHkMn4i","colab_type":"text"},"source":["One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to *basis functions*.\n","\n","One type of basis function is called the Polynomial basis function.\n","The idea is to take our linear model:\n","$$\n","y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n","$$\n","and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.\n","That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.\n","\n","For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:\n","$$\n","y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n","$$\n","Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.\n","What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$.\n","\n","This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"jXIHojh8Mn4k","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","x = np.array([2, 3, 4])\n","poly = PolynomialFeatures(3, include_bias=False)\n","poly.fit_transform(x[:, None])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"s3ygb1oCMn4n","colab_type":"text"},"source":["We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value.\n","This new, higher-dimensional data representation can then be plugged into a linear regression. For instance, let's use a **7th-order polynomial** regression."]},{"cell_type":"code","metadata":{"id":"M7IFZTrTthqm","colab_type":"code","colab":{}},"source":["from sklearn.pipeline import make_pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"2YXuUmmiMn4o","colab_type":"code","colab":{}},"source":["poly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"F2JHt8VVMn4p","colab_type":"text"},"source":["With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$. \n","For example, here is a sine wave with noise:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"uMuo6K1mMn4r","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(1)\n","x = 10 * rng.rand(50)\n","y = np.sin(x) + 0.1 * rng.randn(50)\n","\n","xfit = np.linspace(0,10)\n","\n","poly_model.fit(x[:, np.newaxis], y)\n","yfit = poly_model.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, yfit);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kUWi1m5zMn4t","colab_type":"text"},"source":["Our linear model, through the use of **7th-order polynomial basis functions**, can provide an excellent fit to this non-linear data!"]},{"cell_type":"markdown","metadata":{"id":"dYZ8h0wmXx_4","colab_type":"text"},"source":["# Section 3: Hyperparameters and Model Validation"]},{"cell_type":"markdown","metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"YykuHgFXXx_5","colab_type":"text"},"source":["In the previous section, we saw the basic recipe for applying a linear regression model:\n","\n","1. Choose a class of model\n","2. Choose model hyperparameters\n","3. Fit the model to the training data\n","4. Use the model to predict targets for new data\n","\n","The first two pieces of this—the choice of model and choice of hyperparameters—are perhaps the most important part of using these tools and techniques effectively.\n","In order to make an informed choice, we need a way to *validate* that our model and our hyperparameters are a good fit to the data.\n","While this may sound simple, there are some pitfalls that you must avoid to do this effectively."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2ZVfwxXhXyAl","colab_type":"text"},"source":["## 3.1 Selecting the Best Model\n","\n","Now we will go into a litte more depth on selecting the \"best\" model and hyperparameters.\n","These issues are some of the most important aspects of the practice of machine learning, and I find that this information is often glossed over in introductory machine learning tutorials.\n","\n","Of core importance is the following question: *if our estimator is underperforming, how should we move forward?*\n","There are several possible answers:\n","\n","- Use a more complicated/more flexible model\n","- Use a less complicated/less flexible model\n","- Gather more training samples (increase N)\n","- Gather data to add more features\n","\n","The answer to this question can sometimes be counter-intuitive.\n","In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results!\n","The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SM3VG230XyAn","colab_type":"text"},"source":["## 3.2 The Bias-variance trade-off\n","\n","Fundamentally, the question of \"the best model\" is about finding a sweet spot in the tradeoff between *bias* and *variance*.\n","Consider the following figure, which presents two regression fits to the same dataset:"]},{"cell_type":"code","metadata":{"id":"MOqMdpyjECPt","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import make_pipeline\n","\n","def PolynomialRegression(degree=2, **kwargs):\n","    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n","\n","def make_data(N=30, err=0.8, rseed=1):\n","    # randomly sample the data\n","    rng = np.random.RandomState(rseed)\n","    X = rng.rand(N, 1) ** 2\n","    y = 10 - 1. / (X.ravel() + 0.1)\n","    if err > 0:\n","        y += err * rng.randn(N)\n","    return X, y\n","\n","\n","X, y = make_data()\n","xfit = np.linspace(-0.1, 1.0, 1000)[:, None]\n","model1 = PolynomialRegression(1).fit(X, y)      # first-order fit\n","model20 = PolynomialRegression(20).fit(X, y)    # twentieth-order fit\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","ax[0].scatter(X.ravel(), y, s=40)\n","ax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray')\n","ax[0].axis([-0.1, 1.0, -2, 14])\n","ax[0].set_title('High-bias model: Underfits the data', size=14)\n","\n","ax[1].scatter(X.ravel(), y, s=40)\n","ax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray')\n","ax[1].axis([-0.1, 1.0, -2, 14])\n","ax[1].set_title('High-variance model: Overfits the data', size=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"TpsekzF7XyAo","colab_type":"text"},"source":["It is clear that neither of these models is a particularly good fit to the data, but they fail in different ways.\n","\n","The model on the left attempts to find a straight-line fit through the data.\n","Because the data are intrinsically more complicated than a straight line, the straight-line model will never be able to describe this dataset well.\n","Such a model is said to *underfit* the data: that is, it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high *bias*.\n","\n","The model on the right attempts to fit a high-order polynomial through the data.\n","Here the model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data.\n","Such a model is said to *overfit* the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high *variance*.\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"5PLzM34UXyA2","colab_type":"text"},"source":["We can also visualize our data along with polynomial fits of several degrees:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"M-FuesSMXyA3","colab_type":"code","colab":{}},"source":["X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n","\n","plt.scatter(X.ravel(), y, color='black')\n","axis = plt.axis()\n","for degree in [1, 3, 5]:        # Try first-, third-, and fifth-order fits\n","\n","    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n","    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n","    \n","plt.xlim(-0.1, 1.0)\n","plt.ylim(-2, 12)\n","plt.legend(loc='best');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"neEnhUc2XbT4","colab_type":"text"},"source":["## 3.3 Check-in 3: interactive exercise\n","\n","Play around with model over-fitting in this interactive plot. Write down what you see and your thoughts.\n","* What happens as you increase the order of the polynomial fit?\n","* Do you notice any particular sweet spots, where it looks like the goldilocks zone of capturing the data's behavior without overfitting?"]},{"cell_type":"code","metadata":{"id":"YrlXbFOdTP7R","colab_type":"code","colab":{}},"source":["X, y = make_data()\n","\n","xfit = np.linspace(-0.1, 1.0, 1000)[:, None]\n","df = pd.DataFrame()\n","for i in range (1,21):\n","    model = PolynomialRegression(i).fit(X, y)\n","    yfit = model.predict(xfit)\n","    dfn = pd.DataFrame({'xfit':xfit.ravel(), 'yfit': yfit,  'order' : i})\n","    df = pd.concat([df, dfn])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGZxUXbKU-mv","colab_type":"code","colab":{}},"source":["fig = px.line(\n","    df,\n","    x = 'xfit', \n","    y = 'yfit', \n","    animation_frame = 'order',\n","    animation_group= 'order',\n","    template = 'plotly_dark',\n","    height = 600,\n","    width = 800,\n","    line_shape = 'spline' #key -- or else it will render very funny\n","    )\n","\n","fig.add_trace(\n","    go.Scatter(\n","        x = X.ravel(),\n","        y = y,\n","        mode = 'markers',\n","        showlegend=False, \n","        )\n",")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"rf1T_8rjXx_6","colab_type":"text"},"source":["## 3.4 Thinking about model validation\n","\n","In principle, model validation is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparing the prediction to the known value.\n","\n","The following sections first show a naive approach to model validation and why it\n","fails, before exploring the use of holdout sets and cross-validation for more robust\n","model evaluation."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"aB3KwxhmXx_6","colab_type":"text"},"source":["### 3.4.1 Model validation the wrong way\n","\n","Let's demonstrate the naive approach to validation using the same `X,y` data that we saw in the previous section.\n","\n","Let's create a 20-degree polynomial and see how well it captures the data using **root mean squared error (RMSE)**. The closer to zero, the better the model.\n","\n","**Note**: You can also normalize your RMSE value (e.g. by the mean or standard deviation of `y` data) to compare across datasets. \n","\n"]},{"cell_type":"code","metadata":{"id":"9UZn3Z_0sNXU","colab_type":"code","colab":{}},"source":["from sklearn.metrics import mean_squared_error\n","X, y    = make_data()\n","model   = PolynomialRegression(20)\n","model.fit(X, y)\n","ypred   = model.predict(X)\n","rmse    = mean_squared_error(y, ypred, squared=False)\n","print(\"RMSE = \" + str(rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"MnksZCpBXyAM","colab_type":"text"},"source":["We see a very low normalized RMSE (less than 1), which indicates that the model predicts our data very well!\n","But have we really come upon a model that we expect to be correct almost 100% of the time?\n","\n","As you may have gathered, the answer is no.\n","In fact, this approach contains a fundamental flaw: *it trains and evaluates the model on the same data*.\n","\n","What happens if we only train on the first half of the data, but predict on all the data?"]},{"cell_type":"code","metadata":{"id":"dODUmmraw9Af","colab_type":"code","colab":{}},"source":["model = PolynomialRegression(20)\n","model.fit(X[:X.size//2], y[:X.size//2]) # Fit on the first half of the data\n","ypred   = model.predict(X)\n","rmse    = mean_squared_error(y, ypred, squared=False)\n","print(\"RMSE = \" + str(rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1HpqgdDD0hnL","colab_type":"text"},"source":["Holy cow! Not doing nearly as hot as before **:(**\n","\n","What about if we train on *every other* data point, and still predict back on the entire set? "]},{"cell_type":"code","metadata":{"id":"cFXsGC0dt9GF","colab_type":"code","colab":{}},"source":["model = PolynomialRegression(20)\n","model.fit(X[::2], y[::2])           # Fit on every other point of data\n","ypred   = model.predict(X)\n","rmse    = mean_squared_error(y, ypred, squared=False)\n","print(\"RMSE = \" + str(rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"pmNasS4mXyAN","colab_type":"text"},"source":["### 3.4.2 Model validation the right way: Holdout sets\n","\n","So what can be done?\n","A better sense of a model's performance can be found using what's known as a *holdout set*: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance.\n","This splitting can be done using the ``train_test_split`` utility in Scikit-Learn. Now let's see how well a **third-degree polynomial** regression model performs when we only train on half of the data."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"AEOoU_ZPXyAO","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","# split the data with 50% in each set\n","X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n","                                  train_size=0.5)\n","\n","# fit the model on one set of data\n","model = PolynomialRegression(3) # Fit a 3rd-order model on half the data\n","model.fit(X1, y1)\n","\n","# evaluate the model on the second set of data\n","y2_model = model.predict(X2)\n","rmse    = mean_squared_error(y2, y2_model, squared=False)\n","print(\"RMSE = \" + str(rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"hX5t44J3XyAT","colab_type":"text"},"source":["We see here a much more reasonable result: the performance isn't as good as the 20th-order polynomial trained on the entire dataset, but it's much better at predicting on new data than the 20th-order model trained on only half of the data. \n","\n","The hold-out set is similar to unknown data, because the model has not \"seen\" it before."]},{"cell_type":"markdown","metadata":{"id":"5E53zlBV1TYf","colab_type":"text"},"source":["## 3.5 Check -in 4"]},{"cell_type":"markdown","metadata":{"id":"pq9ZVv0HS_E9","colab_type":"text"},"source":["Using the code cell above, go through a series of degreed polynomials and find the \"best\" model performance based on a half-and-half split of the data."]},{"cell_type":"code","metadata":{"id":"HrTzLKtxTF8x","colab_type":"code","colab":{}},"source":["# WRITE YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gT98CLWnTRUW","colab_type":"text"},"source":["### Answer"]},{"cell_type":"code","metadata":{"id":"vhz5Yr4KTSMN","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","# split the data with 50% in each set\n","X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n","                                  train_size=0.5)\n","\n","# fit the model on one set of data\n","model = PolynomialRegression(5) # Fit a 3rd-order model on half the data\n","model.fit(X1, y1)\n","\n","# evaluate the model on the second set of data\n","y2_model = model.predict(X2)\n","rmse    = mean_squared_error(y2, y2_model, squared=False)\n","print(\"RMSE = \" + str(rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"DXGFbsYvXyAT","colab_type":"text"},"source":["## 3.6 Model validation via cross-validation\n","\n","One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.\n","In the above case, half the dataset does not contribute to the training of the model!\n","This is not optimal, and can cause problems – especially if the initial set of training data is small.\n","\n","One way to address this is to use *cross-validation*; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n","Visually, it might look something like this:"]},{"cell_type":"code","metadata":{"id":"0f0JepYxHoyn","colab_type":"code","colab":{}},"source":["def draw_rects(N, ax, textprop={}):\n","    for i in range(N):\n","        ax.add_patch(plt.Rectangle((0, i), 5, 0.7, fc='black'))\n","        ax.add_patch(plt.Rectangle((5. * i / N, i), 5. / N, 0.7, fc='lightgray'))\n","        ax.text(5. * (i + 0.5) / N, i + 0.35,\n","                \"validation\\nset\", ha='center', va='center', **textprop)\n","        ax.text(0, i + 0.35, \"trial {0}\".format(N - i),\n","                ha='right', va='center', rotation=90, **textprop)\n","    ax.set_xlim(-1, 6)\n","    ax.set_ylim(-0.2, N + 0.2)\n","\n","fig = plt.figure()\n","ax = fig.add_axes([0, 0, 1, 1])\n","ax.axis('off')\n","draw_rects(2, ax, textprop=dict(size=14))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvL2P35MHvNR","colab_type":"text"},"source":["Here we do two validation trials, alternately using each half of the data as a holdout set.\n","Using the split data from before, we could implement it like this:"]},{"cell_type":"code","metadata":{"id":"MXPV_CKz9Tbp","colab_type":"code","colab":{}},"source":["# Let's go back to the 3-degree model\n","model = PolynomialRegression(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"v0u1oOpOXyAV","colab_type":"code","colab":{}},"source":["y2_model = model.fit(X1, y1).predict(X2)\n","y1_model = model.fit(X2, y2).predict(X1)\n","print(mean_squared_error(y2, y2_model, squared=False), ',', \n","      mean_squared_error(y1, y1_model, squared=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"3qPKOvLBXyAY","colab_type":"text"},"source":["What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance.\n","This particular form of cross-validation is a *two-fold cross-validation*—that is, one in which we have split the data into two sets and used each in turn as a validation set.\n","\n","We could expand on this idea to use even more trials, and more folds in the data—for example, here is a visual depiction of five-fold cross-validation:"]},{"cell_type":"code","metadata":{"id":"6f3Y3wOOHztV","colab_type":"code","colab":{}},"source":["fig = plt.figure()\n","ax = fig.add_axes([0, 0, 1, 1])\n","ax.axis('off')\n","draw_rects(5, ax, textprop=dict(size=10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WioN14PHy_Y","colab_type":"text"},"source":["Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n","This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"nR8aT-HAXyAZ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import cross_val_score\n","cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"HS_UYkbqXyAc","colab_type":"text"},"source":["Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.\n","\n","Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the ``cross_validation`` module.\n","For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points: that is, we train on all points but one in each trial.\n","This type of cross-validation is known as *leave-one-out* cross validation, and can be used as follows:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"VU0mgJrVXyAc","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import LeaveOneOut\n","scores = cross_val_score(model, X, y, cv = LeaveOneOut(), \n","                         scoring='neg_root_mean_squared_error')\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1svlHT8n9h0J","colab_type":"text"},"source":["Now let's go to a 4-degree polynomial and see how it performs"]},{"cell_type":"code","metadata":{"id":"Cn7N86jv_KP0","colab_type":"code","colab":{}},"source":["model = PolynomialRegression(4)\n","from sklearn.model_selection import LeaveOneOut\n","scores = cross_val_score(model, X, y, cv = LeaveOneOut(), \n","                         scoring='neg_root_mean_squared_error')\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"dGy3GSfhXyAe","colab_type":"text"},"source":["Because we have 30 samples, the leave one out cross-validation yields scores for 30 trials, and the score indicates either small error or larger error.\n","Taking the mean of these gives an estimate of the error rate:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"gmD7CfnbXyAf","colab_type":"code","colab":{}},"source":["scores.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyUCH7tkDONH","colab_type":"text"},"source":["To look at this in another light, consider what happens if we use these two models to predict the y-value for some new data.\n","In the following diagrams, the red/lighter points indicate data that is omitted from the training set:"]},{"cell_type":"code","metadata":{"id":"01ND5QBQ_S-F","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","X, y = make_data()\n","X2, y2 = make_data(10, rseed=42)\n","\n","ax[0].scatter(X.ravel(), y, s=40, c='blue')\n","ax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray')\n","ax[0].axis([-0.1, 1.0, -2, 14])\n","ax[0].set_title('High-bias model: Underfits the data', size=14)\n","ax[0].scatter(X2.ravel(), y2, s=40, c='red')\n","ax[0].text(0.02, 0.98, \n","           \"training score: $RMSE$ = {0:.2f}\".format(mean_squared_error(model1.predict(X), y, squared=False)),\n","           ha='left', va='top', transform=ax[0].transAxes, size=14, color='blue')\n","ax[0].text(0.02, 0.91, \n","           \"training score: $RMSE$ = {0:.2f}\".format(mean_squared_error(model1.predict(X2), y2, squared=False)),\n","           ha='left', va='top', transform=ax[0].transAxes, size=14, color='red')\n","\n","ax[1].scatter(X.ravel(), y, s=40, c='blue')\n","ax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray')\n","ax[1].axis([-0.1, 1.0, -2, 14])\n","ax[1].set_title('High-variance model: Overfits the data', size=14)\n","ax[1].scatter(X2.ravel(), y2, s=40, c='red')\n","ax[1].text(0.02, 0.98, \n","           \"training score: $RMSE$ = {0:.2g}\".format(mean_squared_error(model20.predict(X), y, squared=False)),\n","           ha='left', va='top', transform=ax[1].transAxes, size=14, color='blue')\n","ax[1].text(0.02, 0.91, \n","           \"training score: $RMSE$ = {0:.2g}\".format(mean_squared_error(model20.predict(X2), y2, squared=False)),\n","           ha='left', va='top', transform=ax[1].transAxes, size=14, color='red')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBT4KkdP_q6v","colab_type":"text"},"source":["Let's also assess our models on the measurement of $R^2$, or [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination). This statistic measures how well a model performs relative to a simple mean of the target values. $R^2=1$ indicates a perfect match, $R^2=0$ indicates the model does no better than simply taking the mean of the data, and negative values mean even worse models."]},{"cell_type":"code","metadata":{"id":"49lNpscz_fLY","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","X, y = make_data()\n","X2, y2 = make_data(10, rseed=42)\n","\n","ax[0].scatter(X.ravel(), y, s=40, c='blue')\n","ax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray')\n","ax[0].axis([-0.1, 1.0, -2, 14])\n","ax[0].set_title('High-bias model: Underfits the data', size=14)\n","ax[0].scatter(X2.ravel(), y2, s=40, c='red')\n","ax[0].text(0.02, 0.98, \"training score: $R^2$ = {0:.2f}\".format(model1.score(X, y)),\n","           ha='left', va='top', transform=ax[0].transAxes, size=14, color='blue')\n","ax[0].text(0.02, 0.91, \"validation score: $R^2$ = {0:.2f}\".format(model1.score(X2, y2)),\n","           ha='left', va='top', transform=ax[0].transAxes, size=14, color='red')\n","\n","ax[1].scatter(X.ravel(), y, s=40, c='blue')\n","ax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray')\n","ax[1].axis([-0.1, 1.0, -2, 14])\n","ax[1].set_title('High-variance model: Overfits the data', size=14)\n","ax[1].scatter(X2.ravel(), y2, s=40, c='red')\n","ax[1].text(0.02, 0.98, \"training score: $R^2$ = {0:.2g}\".format(model20.score(X, y)),\n","           ha='left', va='top', transform=ax[1].transAxes, size=14, color='blue')\n","ax[1].text(0.02, 0.91, \"validation score: $R^2$ = {0:.2g}\".format(model20.score(X2, y2)),\n","           ha='left', va='top', transform=ax[1].transAxes, size=14, color='red')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"zeSXUpppXyAo","colab_type":"text"},"source":["From the scores associated with these two models, we can make an observation that holds more generally:\n","\n","- For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.\n","- For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"VfGUxC9uXyAk","colab_type":"text"},"source":["Other cross-validation schemes can be used similarly.\n","For a description of what is available in Scikit-Learn, use IPython to explore the ``sklearn.cross_validation`` submodule, or take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html).\n","\n","If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in the following figure:"]},{"cell_type":"code","metadata":{"id":"IDbk6E0wGnKd","colab_type":"code","colab":{}},"source":["x = np.linspace(0, 1, 1000)\n","y1 = -(x - 0.5) ** 2\n","y2 = y1 - 0.33 + np.exp(x - 1)\n","\n","fig, ax = plt.subplots()\n","ax.plot(x, y2, lw=10, alpha=0.5, color='blue')\n","ax.plot(x, y1, lw=10, alpha=0.5, color='red')\n","\n","ax.text(0.15, 0.2, \"training score\", rotation=45, size=16, color='blue')\n","ax.text(0.2, -0.05, \"validation score\", rotation=20, size=16, color='red')\n","\n","ax.text(0.02, 0.1, r'$\\longleftarrow$ High Bias', size=18, rotation=90, va='center')\n","ax.text(0.98, 0.1, r'$\\longleftarrow$ High Variance $\\longrightarrow$', size=18, rotation=90, ha='right', va='center')\n","ax.text(0.48, -0.12, 'Best$\\\\longrightarrow$\\nModel', size=18, rotation=90, va='center')\n","\n","ax.set_xlim(0, 1)\n","ax.set_ylim(-0.3, 0.5)\n","\n","ax.set_xlabel(r'model complexity $\\longrightarrow$', size=14)\n","ax.set_ylabel(r'model score $\\longrightarrow$', size=14)\n","\n","ax.xaxis.set_major_formatter(plt.NullFormatter())\n","ax.yaxis.set_major_formatter(plt.NullFormatter())\n","\n","ax.set_title(\"Validation Curve Schematic\", size=16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_CEjXq2GoXx","colab_type":"text"},"source":["The diagram shown here is often called a *validation curve*, and we see the following essential features:\n","\n","- The training score is everywhere higher than the validation score; the model will generally be a better fit to data it has seen than to data it has not seen.\n","- For very low model complexity (a high-bias model), the training data is under-fit, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n","- For very high model complexity (a high-variance model), the training data is over-fit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n","- For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance.\n","\n","The means of tuning the model complexity varies from model to model; when we discuss individual models in depth in later sections, we will see how each model allows for such tuning."]},{"cell_type":"markdown","metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"qr-CZgy6XyAr","colab_type":"text"},"source":["## 3.7 Validation curves in Scikit-Learn\n","\n","Let's look at an example of using cross-validation to compute the validation curve for a class of models.\n","Here we will use a *polynomial regression* model: a generalized linear model in which the degree of the polynomial is a tunable parameter.\n","For example, a degree-1 polynomial fits a straight line to the data; for model parameters $a$ and $b$:\n","\n","$$\n","y = ax + b\n","$$\n","\n","A degree-3 polynomial fits a cubic curve to the data; for model parameters $a, b, c, d$:\n","\n","$$\n","y = ax^3 + bx^2 + cx + d\n","$$\n","\n","We can generalize this to any number of polynomial features.\n","In Scikit-Learn, we can implement this with a simple linear regression combined with the polynomial preprocessor.\n","We will use a *pipeline* to string these operations together ."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"zgP-JlMrXyA5","colab_type":"text"},"source":["The knob controlling model complexity in this case is the degree of the polynomial, which can be any non-negative integer.\n","A useful question to answer is this: what degree of polynomial provides a suitable trade-off between bias (under-fitting) and variance (over-fitting)?\n","\n","We can make progress in this by visualizing the validation curve for this particular data and model; this can be done straightforwardly using the ``validation_curve`` convenience routine provided by Scikit-Learn.\n","Given a model, data, parameter name, and a range to explore, this function will automatically compute both the training score and validation score across the range:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Q4koAaTkXyA6","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import validation_curve\n","degree = np.arange(0, 21)\n","train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n","                                          'polynomialfeatures__degree', degree, cv=5)\n","\n","plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n","plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n","plt.legend(loc='best')\n","plt.ylim(0, 1)\n","plt.xlabel('degree')\n","plt.ylabel('score');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"u82eAS2hXyA9","colab_type":"text"},"source":["This shows precisely the qualitative behavior we expect: the training score is everywhere higher than the validation score; the training score is monotonically improving with increased model complexity; and the validation score reaches a maximum before dropping off as the model becomes over-fit.\n","\n","From the validation curve, we can read-off that the optimal trade-off between bias and variance is found for a third-order polynomial; we can compute and display this fit over the original data as follows:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"auWo9hL_XyA_","colab_type":"code","colab":{}},"source":["plt.scatter(X.ravel(), y)\n","lim = plt.axis()\n","y_test = PolynomialRegression(5).fit(X, y).predict(X_test)\n","plt.plot(X_test.ravel(), y_test);\n","plt.axis(lim);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"xnxpIjcDXyBD","colab_type":"text"},"source":["Notice that finding this optimal model did not actually require us to compute the training score, but examining the relationship between the training score and validation score can give us useful insight into the performance of the model."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2MivgaVBXyBE","colab_type":"text"},"source":["## 3.8 EXTRA - The learning curve\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SGDK0l18XyBM","colab_type":"text"},"source":["We will duplicate the preceding code to plot the validation curve for this larger dataset; for reference let's over-plot the previous results as well:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"3gUzcVl0XyBK","colab_type":"code","colab":{}},"source":["X2, y2 = make_data(200)\n","plt.scatter(X2.ravel(), y2);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F78qjHlB_Za8","colab_type":"code","colab":{}},"source":["degree = np.arange(21)\n","train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,\n","                                            'polynomialfeatures__degree', degree, cv=5)\n","\n","plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n","plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n","plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')\n","plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n","plt.legend(loc='lower center')\n","plt.ylim(0, 1)\n","plt.xlabel('degree')\n","plt.ylabel('score');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SwEecdJAXyBP","colab_type":"text"},"source":["The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset.\n","It is clear from the validation curve that the larger dataset can support a much more complicated model: the peak here is probably around a degree of 6, but even a degree-20 model is not seriously over-fitting the data—the validation and training scores remain very close.\n","\n","Thus we see that the behavior of the validation curve has not one but two important inputs: the model complexity and the number of training points.\n","It is often useful to to explore the behavior of the model as a function of the number of training points, which we can do by using increasingly larger subsets of the data to fit our model.\n","A plot of the training/validation score with respect to the size of the training set is known as a *learning curve.*\n","\n","The general behavior we would expect from a learning curve is this:\n","\n","- A model of a given complexity will *overfit* a small dataset: this means the training score will be relatively high, while the validation score will be relatively low.\n","- A model of a given complexity will *underfit* a large dataset: this means that the training score will decrease, but the validation score will increase.\n","- A model will never, except by chance, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross.\n","\n","With these features in mind, we would expect a learning curve to look qualitatively like that shown in the following figure:"]},{"cell_type":"code","metadata":{"id":"VWl34CKOHdwE","colab_type":"code","colab":{}},"source":["N = np.linspace(0, 1, 1000)\n","y1 = 0.75 + 0.2 * np.exp(-4 * N)\n","y2 = 0.7 - 0.6 * np.exp(-4 * N)\n","\n","fig, ax = plt.subplots()\n","ax.plot(x, y1, lw=10, alpha=0.5, color='blue')\n","ax.plot(x, y2, lw=10, alpha=0.5, color='red')\n","\n","ax.text(0.2, 0.88, \"training score\", rotation=-10, size=16, color='blue')\n","ax.text(0.2, 0.5, \"validation score\", rotation=30, size=16, color='red')\n","\n","ax.text(0.98, 0.45, r'Good Fit $\\longrightarrow$', size=18, rotation=90, ha='right', va='center')\n","ax.text(0.02, 0.57, r'$\\longleftarrow$ High Variance $\\longrightarrow$', size=18, rotation=90, va='center')\n","\n","ax.set_xlim(0, 1)\n","ax.set_ylim(0, 1)\n","\n","ax.set_xlabel(r'training set size $\\longrightarrow$', size=14)\n","ax.set_ylabel(r'model score $\\longrightarrow$', size=14)\n","\n","ax.xaxis.set_major_formatter(plt.NullFormatter())\n","ax.yaxis.set_major_formatter(plt.NullFormatter())\n","\n","ax.set_title(\"Learning Curve Schematic\", size=16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"JPEYoXr4XyBQ","colab_type":"text"},"source":["The notable feature of the learning curve is the convergence to a particular score as the number of training samples grows.\n","In particular, once you have enough points that a particular model has converged, *adding more training data will not help you!*\n","The only way to increase model performance in this case is to use another (often more complex) model."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"6IG4PB18XyBS","colab_type":"text"},"source":["### 3.8.1 Learning curves in Scikit-Learn\n","\n","Scikit-Learn offers a convenient utility for computing such learning curves from your models; here we will compute a learning curve for our original dataset with a second-order polynomial model and a ninth-order polynomial:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"RLEd0teBXyBS","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import learning_curve\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","\n","X,y = make_data(N=40)\n","\n","for i, degree in enumerate([2, 5]):\n","    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n","                                         X, y, cv=7,\n","                                         train_sizes=np.linspace(0.3, 1, 25))\n","\n","    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n","    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n","    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n","                 color='gray', linestyle='dashed')\n","\n","    ax[i].set_ylim(0, 1.1)\n","    ax[i].set_xlim(N[0], N[-1])\n","    ax[i].set_xlabel('training size')\n","    ax[i].set_ylabel('score')\n","    ax[i].set_title('degree = {0}'.format(degree), size=14)\n","    ax[i].legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ObKLHxBOXyBV","colab_type":"text"},"source":["This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data.\n","In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) *adding more training data will not significantly improve the fit!*\n","This situation is seen in the left panel, with the learning curve for the degree-2 model.\n","\n","The only way to increase the converged score is to use a different (usually more complicated) model.\n","We see this in the right panel: by moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance (indicated by the difference between the training and validation scores).\n","If we were to add even more data points, the learning curve for the more complicated model would eventually converge.\n","\n","Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"3MIfhcVGXyBW","colab_type":"text"},"source":["## 3.9 EXTRA - Validation in Practice: Grid Search\n","\n","The preceding discussion is meant to give you some intuition into the trade-off between bias and variance, and its dependence on model complexity and training set size.\n","In practice, models generally have more than one knob to turn, and thus plots of validation and learning curves change from lines to multi-dimensional surfaces.\n","In these cases, such visualizations are difficult and we would rather simply find the particular model that maximizes the validation score.\n","\n","Scikit-Learn provides automated tools to do this in the grid search module.\n","Here is an example of using grid search to find the optimal polynomial model.\n","We will explore a three-dimensional grid of model features; namely the polynomial degree, the flag telling us whether to fit the intercept, and the flag telling us whether to normalize the problem.\n","This can be set up using Scikit-Learn's ``GridSearchCV`` meta-estimator:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Mqv65LYyXyBX","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {'polynomialfeatures__degree': np.arange(21),\n","              'linearregression__fit_intercept': [True, False],\n","              'linearregression__normalize': [True, False]}\n","\n","grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"hGrhB5SOXyBY","colab_type":"text"},"source":["Notice that like a normal estimator, this has not yet been applied to any data.\n","Calling the ``fit()`` method will fit the model at each grid point, keeping track of the scores along the way:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"QuaTXQpQXyBb","colab_type":"code","colab":{}},"source":["grid.fit(X, y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SkZ_lymYXyBg","colab_type":"text"},"source":["Now that this is fit, we can ask for the best parameters as follows:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"c0Tv-EbOXyBh","colab_type":"code","colab":{}},"source":["grid.best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"BkuDczZXXyBm","colab_type":"text"},"source":["Finally, if we wish, we can use the best model and show the fit to our data using code from before:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Fc7TEzHRXyBm","colab_type":"code","colab":{}},"source":["model = grid.best_estimator_\n","\n","plt.scatter(X.ravel(), y)\n","lim = plt.axis()\n","y_test = model.fit(X, y).predict(X_test)\n","plt.plot(X_test.ravel(), y_test)\n","plt.axis(lim);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7yZnSyFCXyBo","colab_type":"text"},"source":["The grid search provides many more options, including the ability to specify a custom scoring function, to parallelize the computations, to do randomized searches, and more.\n","For information, see the examples in Scikit-Learn's [grid search documentation](http://Scikit-Learn.org/stable/modules/grid_search.html)."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KIp0Za4XXyBp","colab_type":"text"},"source":["## 3.10 Summary\n","\n","In this section, we have begun to explore the concept of **model validation** and **hyperparameter optimization**, focusing on intuitive aspects of the bias–variance trade-off and how it comes into play when fitting models to data.\n","\n","In particular, we found that the use of a **validation set** or **cross-validation** approach is *vital* when tuning parameters in order to avoid over-fitting.\n","\n","Next, we want to show you how you can incorporate even more sophisticated statistical software into your research methods, straight from `R`! If you're not familiar with `R`, there are some slight syntax differences, but hopefully the code doesn't look too strange. "]},{"cell_type":"markdown","metadata":{"id":"LUpw7sWHR1Zq","colab_type":"text"},"source":["# Section 4: Mixing methods in Python and R"]},{"cell_type":"markdown","metadata":{"id":"9ZHL4dP6S_Tm","colab_type":"text"},"source":["In this section we'll learn how to use **both** R and Python in your data analysis through the tool of the `rpy2` package.\n","\n","*rpy2* is running an embedded R, providing access to it from Python using R’s own C-API through either:\n","\n","* a high-level interface making R functions an objects just like Python functions and providing a seamless conversion to numpy and pandas data structures\n","* a low-level interface closer to the C-API\n","\n","It allows us to intertwine `R` and `Python` together in the same notebook. It is also providing features for when working with jupyter notebooks or ipython."]},{"cell_type":"code","metadata":{"id":"XGT-SpYAi16T","colab_type":"code","colab":{}},"source":["%%capture\n","\n","%load_ext rpy2.ipython\n","from rpy2.robjects.packages import importr\n","utils = importr('utils')\n","### this install takes a second. it looks like it is hanging up, but then it works ###\n","utils.install_packages('ggplot2',  repos = 'https://cloud.r-project.org')\n","utils.install_packages('tidyverse',  repos = 'https://cloud.r-project.org')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXtvZnBlXMLl","colab_type":"text"},"source":["## 4.1 Turning a cell into R"]},{"cell_type":"markdown","metadata":{"id":"FNFiPWluXQOC","colab_type":"text"},"source":["We can turn a line of code into `R` using the `%R` cell magic command. We can turn an *entire code cell* into `R` by putting the cell magic command `%%R` at the top of the cell (notice the double `%`s)."]},{"cell_type":"code","metadata":{"id":"Kw94taw7Vf-V","colab_type":"code","colab":{}},"source":["v_ex_py = np.array([1, 2, 3, 4, 5])\n","%R v_ex_r <- c(1, 2, 3, 4, 5) # Set a line to R"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1vACL3fV_y_","colab_type":"code","colab":{}},"source":["# For referecne, uncomment the line below to try R syntax without specifying %R\n","\n","v_ex_r <- c(1, 2, 3, 4, 5) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jK_yKmdgi2Av","colab_type":"code","colab":{}},"source":["%%R # Set the whole cell to R\n","\n","x_vals <- c(1, 2, 3, 4, 5)\n","y_vals <- c(1, 2, 4, 8, 16)\n","\n","plot(x_vals, y_vals, col='purple', pch=12, main='R plot in Python')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQHaN2OaWzw-","colab_type":"text"},"source":["## 4.2 Passing data back and forth"]},{"cell_type":"markdown","metadata":{"id":"vkzbxE_wW3kj","colab_type":"text"},"source":["We can pass variables between the `R` and `Python` cells by using the `%%R -i` and `%%R -o` commands."]},{"cell_type":"code","metadata":{"id":"nwILA1ybi19p","colab_type":"code","colab":{}},"source":["df_AB = pd.DataFrame({'A':(np.random.randint(-2, 2, 10)+np.arange(10)).tolist(),\n","                      'B':(np.random.randint(-2, 2, 10)+np.arange(10)).tolist()})\n","df_AB.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOUiChPwXBi4","colab_type":"text"},"source":["Let's pass this `DataFrame` into an `R` cell and plot it. "]},{"cell_type":"code","metadata":{"id":"nsj1Kv59jxET","colab_type":"code","colab":{}},"source":["%%R -i df_AB\n","\n","plot(df_AB, main='Plotting a Python DF in R', col='purple', pch=9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iOJpylTEXHr9","colab_type":"text"},"source":["We can also output objects from `R` into the `Python` environment. "]},{"cell_type":"code","metadata":{"id":"lGoiJER4kDwf","colab_type":"code","colab":{}},"source":["%%R -o model,coef \n","          # ^ Do not leave a space between your commas\n","model <- lm(B ~ A, data = df_AB)\n","coef <- model$coefficients"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDOQAjiQkj-P","colab_type":"code","colab":{}},"source":["print(model)\n","print(type(model))\n","\n","print(coef)\n","print(type(coef))\n","\n","print(list(coef))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHKc9Zg-XoXH","colab_type":"text"},"source":["With the use of both of these environments, we can take advantage of what both langauges have to offer. For instance, we can take a dataset from `R` and throw it into our `Python` environment to plot it up with a familiar `seaborn` command."]},{"cell_type":"code","metadata":{"id":"EtCF063FlU4D","colab_type":"code","colab":{}},"source":["%%R -o cars_df\n","\n","library(datasets)\n","cars_df <- cars"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWWN3wj2ldXu","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","print(cars_df.head())\n","\n","sns.pairplot(x_vars=['speed'], y_vars=['dist'], data=cars_df, height=5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeLKk3xWloI1","colab_type":"code","colab":{}},"source":["%%R \n","r_var <- c(1,2,3,4,5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lene8z1sX3HH","colab_type":"text"},"source":["We can also pass variables back and forth with the single-line `%Rget` and `%Rpush` commands. These commands are one line only (notice the single % sign), so the rest of the cell stays in the `Python` environment. "]},{"cell_type":"code","metadata":{"id":"3Odiox4Al8Ec","colab_type":"code","colab":{}},"source":["python_var = %Rget r_var\n","\n","print(python_var)\n","print(type(python_var))\n","print(list(python_var))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PopIv_EKl8Lc","colab_type":"code","colab":{}},"source":["my_python_var1 = np.array([1, 2, 3, 4, 5])\n","my_python_var2 = np.array([1, 2, 4, 8, 16])\n","\n","%Rpush my_python_var1 my_python_var2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XUVQyeul8JJ","colab_type":"code","colab":{}},"source":["%%R\n","\n","plot(my_python_var1, my_python_var2, col='purple', \n","     pch=16, main='Python objects pushed to R')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4mvEl5WYKOq","colab_type":"text"},"source":["## 4.3 `R` libraries"]},{"cell_type":"markdown","metadata":{"id":"LEtGMSmWYM3c","colab_type":"text"},"source":["With this package, we can also take advantage of the landscape of libraries that `R` offers."]},{"cell_type":"code","metadata":{"id":"_8BYJVNZo6y7","colab_type":"code","colab":{}},"source":["## Import Librarys\n","\n","%%capture\n","%%R\n","library(tidyverse)\n","library(broom)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aC6iTl7RnPt_","colab_type":"code","colab":{}},"source":["df = pd.DataFrame({'Letter':['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],\n","                      'X':(np.random.randint(1, 10, 9)).tolist(),\n","                      'Y':(np.random.randint(1, 10, 9)).tolist(),\n","                      'Z':(np.random.randint(1, 10, 9)).tolist()})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iXuLpwQxYc7F","colab_type":"text"},"source":["Let's use the famous `ggplot`."]},{"cell_type":"code","metadata":{"id":"_8KFvx7InPrI","colab_type":"code","colab":{}},"source":["%%R -i df\n","ggplot(data = df) + geom_point(aes(x=X, y=Y, color=Letter, size=Z))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qa1Z3SzmpWbA","colab_type":"code","colab":{}},"source":["x = np.array([1, 2, 4, 6, 5, 8])\n","y = np.array([0, 1, 3, 2, 5, 7])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nQOcbuV9YfeA","colab_type":"text"},"source":["We can also use the regression libraries that make `R` such a popular language in data science."]},{"cell_type":"code","metadata":{"id":"GgrKztYtpqNO","colab_type":"code","colab":{}},"source":["%%R -i x,y -o my_coef\n","\n","xylm = lm(y~x)\n","my_coef = coef(xylm)\n","par(mfrow = c(2, 2))\n","plot(xylm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArRBN7KjqYwX","colab_type":"code","colab":{}},"source":["print(my_coef)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7RlMKJNSYG5-","colab_type":"text"},"source":["Now let's try a more advanced example using the Gapminder data we visualized yesterday."]},{"cell_type":"markdown","metadata":{"id":"V0Fo03ABbbOK","colab_type":"text"},"source":["## EXTRA - 4.4 Advanced example: Gapminder data"]},{"cell_type":"markdown","metadata":{"id":"gBH_xovRYMnC","colab_type":"text"},"source":["First, we'll load the data again."]},{"cell_type":"code","metadata":{"id":"mXsUvLorblaC","colab_type":"code","colab":{}},"source":["gapminder = px.data.gapminder()\n","px.scatter(gapminder, x=\"gdpPercap\", y=\"lifeExp\", animation_frame=\"year\", animation_group=\"country\",\n","           size=\"pop\", color=\"country\", hover_name=\"country\", \n","           log_x = True, \n","           size_max=45, range_x=[100,100000], range_y=[25,90])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyVPiDoVch-9","colab_type":"text"},"source":["### 4.4.1 Stardard Ordinary Least Squares (OLS) model from `R`"]},{"cell_type":"markdown","metadata":{"id":"i-Z0fK4fYQoV","colab_type":"text"},"source":["Let's use the popular OLS library from `R` to create a linear model, like we did earlier with Sci-KitLearn."]},{"cell_type":"code","metadata":{"id":"1xy_jAeBb2if","colab_type":"code","colab":{}},"source":["%%R -i gapminder\n","m1.ols <- lm(lifeExp ~ country + gdpPercap + pop, data = gapminder)\n","summary(m1.ols)$coefficients[c('gdpPercap', 'pop'),]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1HBh164_YbUQ","colab_type":"text"},"source":["Notice that with this `R` package we are able to get statistics on our coefficients, something that we couldn't do in SciKitLearn. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"NvX3vHTxgIlu","colab_type":"text"},"source":["### 4.4.3 Now let's use a different library, and include the `lfe` module \n","\n","**Note** (standard errors are different)"]},{"cell_type":"code","metadata":{"id":"xpHTYtjtY0XH","colab_type":"code","colab":{}},"source":["%%capture\n","utils.install_packages('lfe',  repos = 'https://cloud.r-project.org')\n","%R library(lfe)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIV2nwDzctc3","colab_type":"code","colab":{}},"source":["%%R\n","m1.lfe <- felm(lifeExp ~ gdpPercap + pop | country, data = gapminder)\n","\n","std.errs <- cbind(\n","  summary(m1.ols)$coefficients[c('gdpPercap', 'pop'),2],\n","  summary(m1.lfe)$coefficients[,2]\n",")\n","colnames(std.errs) <- c('OLS w/ Intercepts', 'felm Model')\n","std.errs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctwzD3xvhWQd","colab_type":"text"},"source":["### 4.4.4 And now with the `plm` module "]},{"cell_type":"code","metadata":{"id":"pobtI8cpY2zD","colab_type":"code","colab":{}},"source":["%%capture\n","utils.install_packages('plm',  repos = 'https://cloud.r-project.org')\n","%R library(plm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibw_2S_ugPDz","colab_type":"code","colab":{}},"source":["%%R\n","m1.plm <- plm(lifeExp ~ gdpPercap + pop, data = gapminder, model = 'within', index = c('country'))\n","\n","summary(m1.plm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rPrEfawZLkA","colab_type":"text"},"source":["## 4.5 Summary"]},{"cell_type":"markdown","metadata":{"id":"vJQWeFMGao1y","colab_type":"text"},"source":["We made it through another day!\n","\n","Let's summarize. Today we've learned:\n","\n","* How to use SciKitLearn to create regression models in Python\n","* The choise of model **and** hyperparameters is crucial to determining the \"best\" model for your data\n","* Validation set and cross-validation are an invaluable test for machine learning.\n","* There are advantages to using `Python` and `R`, and we can get the best of both worlds by combining them in out data analysis practices.\n","\n","Obviously we couldn't teach it all in this one class, so here are some more links to help learn about the capabilities of the SciKitLearn and `rpy2` packages."]},{"cell_type":"markdown","metadata":{"id":"6kB3l6ITsiyQ","colab_type":"text"},"source":["**Links used**\n","* https://www.linkedin.com/pulse/interfacing-r-from-python-3-jupyter-notebook-jared-stufft/\n","* https://blog.revolutionanalytics.com/2016/01/pipelining-r-python.html\n","\n","**`rpy2` documentation**: \n","* https://rpy2.github.io/doc/latest/html/index.html\n","* https://rpy2.github.io/doc/v3.0.x/html/interactive.html#module-rpy2.ipython.rmagic\n","\n","**R-magic documentation** \n","* https://ipython.org/ipython-doc/2/config/extensions/rmagic.html"]},{"cell_type":"markdown","metadata":{"id":"sz_FNAYrGUXR","colab_type":"text"},"source":["--------------------\n","--------------------\n","--------------------"]},{"cell_type":"markdown","metadata":{"id":"ejY_UnnRIhMv","colab_type":"text"},"source":["# Section 5: EXTRAS\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-0Bu6rWj6X2A","colab_type":"text"},"source":["**Warning**: The following sections are in no particular order, but are some extra sections if you're interested in learning more about SciKitLearn and linear models in Python."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"x4LZ7Gb1Mn5E","colab_type":"text"},"source":["## 5.1 Example: Predicting Bicycle Traffic"]},{"cell_type":"markdown","metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"VTZ0QZllMn5E","colab_type":"text"},"source":["As an example, let's take a look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n","\n","In this section, we will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor.\n","We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day.\n","\n","In particular, this is an example of how the tools of Scikit-Learn can be used in a statistical modeling framework, in which the parameters of the model are assumed to have interpretable meaning.\n","As discussed previously, this is not a standard approach within machine learning, but such interpretation is possible for some models.\n","\n","Let's start by loading the two datasets, indexing by date:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"82dTiz9CMn5H","colab_type":"code","colab":{}},"source":["counts = pd.read_csv(loaddir+'/Fremont_Bridge_Hourly_Bicycle_Counts_by_Month_October_2012_to_present.csv', index_col='Date', parse_dates=True)\n","weather = pd.read_csv(loaddir+'/bike_weather.csv', index_col = 'DATE', parse_dates=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_x3OEad7xNI","colab_type":"code","colab":{}},"source":["daily = counts.resample('d').sum()\n","daily.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"uqDtqb5eMn5J","colab_type":"text"},"source":["Next we will compute the total daily bicycle traffic, and put this in its own dataframe:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"AZAawrsxMn5K","colab_type":"code","colab":{}},"source":["daily = counts.resample('d').sum()\n","daily['Total'] = daily.sum(axis=1)\n","daily = daily[['Total']] # remove other columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"svr3Xg5xMn5M","colab_type":"text"},"source":["We saw previously that the patterns of use generally vary from day to day; let's account for this in our data by adding binary columns that indicate the day of the week:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"s-0OtcDKMn5M","colab_type":"code","colab":{}},"source":["days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n","for i in range(7):\n","    daily[days[i]] = (daily.index.dayofweek == i).astype(float)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"5CsV9DKNMn5Q","colab_type":"text"},"source":["Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"B5H25rlAMn5R","colab_type":"code","colab":{}},"source":["from pandas.tseries.holiday import USFederalHolidayCalendar\n","cal = USFederalHolidayCalendar()\n","holidays = cal.holidays('2012', '2016')\n","daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n","daily['holiday'].fillna(0, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kIMQKRDzMn5V","colab_type":"text"},"source":["We also might suspect that the hours of daylight would affect how many people ride; let's use the standard astronomical calculation to add this information:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"ORpDG1GuMn5W","colab_type":"code","colab":{}},"source":["def hours_of_daylight(date, axis=23.44, latitude=47.61):\n","    \"\"\"Compute the hours of daylight for the given date\"\"\"\n","    days = (date - pd.datetime(2000, 12, 21)).days\n","    m = (1. - np.tan(np.radians(latitude))\n","         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n","    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n","\n","daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n","daily[['daylight_hrs']].plot()\n","plt.ylim(8, 17)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"GAwqooopMn5Y","colab_type":"text"},"source":["We can also add the average temperature and total precipitation to the data.\n","In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"0jq8Wsc_Mn5Z","colab_type":"code","colab":{}},"source":["# temperatures are in 1/10 deg C; convert to C\n","weather['TMIN'] /= 10\n","weather['TMAX'] /= 10\n","weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n","\n","# precip is in 1/10 mm; convert to inches\n","weather['PRCP'] /= 254\n","weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n","\n","daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])\n","daily.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"pLjI--BOMn5a","colab_type":"text"},"source":["Finally, let's add a counter that increases from day 1, and measures how many years have passed.\n","This will let us measure any observed annual increase or decrease in daily crossings:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"C9aXGBJ8Mn5b","colab_type":"code","colab":{}},"source":["daily['annual'] = (daily.index - daily.index[0]).days / 365."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"CrmTUp8YMn5i","colab_type":"text"},"source":["Now our data is in order, and we can take a look at it:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"qr1YHaQHMn5i","colab_type":"code","colab":{}},"source":["daily.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"tPR4PFCJMn5k","colab_type":"text"},"source":["With this in place, we can choose the columns to use, and fit a linear regression model to our data.\n","We will set ``fit_intercept = False``, because the daily flags essentially operate as their own day-specific intercepts:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"yaKCI3k9Mn5l","colab_type":"code","colab":{}},"source":["# Drop any rows with null values\n","daily.dropna(axis=0, how='any', inplace=True)\n","\n","column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n","                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n","X = daily[column_names]\n","y = daily['Total']\n","\n","model = LinearRegression(fit_intercept=False)\n","model.fit(X, y)\n","daily['predicted'] = model.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"VKV20jwKMn5m","colab_type":"text"},"source":["Finally, we can compare the total and predicted bicycle traffic visually:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"jtx1ueRjMn5m","colab_type":"code","colab":{}},"source":["daily[['Total', 'predicted']].plot(alpha=0.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"bvEN5x4SMn5p","colab_type":"text"},"source":["It is evident that we have missed some key features, especially during the summer time.\n","Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures).\n","Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"27t3bGq4Mn5q","colab_type":"code","colab":{}},"source":["params = pd.Series(model.coef_, index=X.columns)\n","params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"5sbrOAEjMn5t","colab_type":"text"},"source":["These numbers are difficult to interpret without some measure of their uncertainty.\n","We can compute these uncertainties quickly using bootstrap resamplings of the data:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"-JxQkJipMn5t","colab_type":"code","colab":{}},"source":["from sklearn.utils import resample\n","np.random.seed(1)\n","err = np.std([model.fit(*resample(X, y)).coef_\n","              for i in range(1000)], 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Iz7bsM_BMn5x","colab_type":"text"},"source":["With these errors estimated, let's again look at the results:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"gfwWjpbzMn5y","colab_type":"code","colab":{}},"source":["print(pd.DataFrame({'effect': params.round(0),\n","                    'error': err.round(0)}))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"3PptIlSsMn50","colab_type":"text"},"source":["We first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays.\n","We see that for each additional hour of daylight, 129 ± 9 more people choose to ride; a temperature increase of one degree Celsius encourages 65 ± 4 people to grab their bicycle; a dry day means an average of 548 ± 33 more riders, and each inch of precipitation means 665 ± 62 more people leave their bike at home.\n","Once all these effects are accounted for, we see a modest increase of 27 ± 18 new daily riders each year.\n","\n","Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation *and* cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.\n","Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days).\n","These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!"]},{"cell_type":"markdown","metadata":{"id":"mk0uR5s13wh2","colab_type":"text"},"source":["## 5.2 Preprocessing your data\n","* https://scikit-learn.org/stable/modules/preprocessing.html\n","* https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n","* https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"F_PZq18JMn4u","colab_type":"text"},"source":["## 5.3 Basis functions and Gaussian models\n","\n","Of course, other basis functions are possible.\n","For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases.\n","The result might look something like the following figure:"]},{"cell_type":"code","metadata":{"id":"K3DJ1JF3_Ruk","colab_type":"code","colab":{}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class GaussianFeatures(BaseEstimator, TransformerMixin):\n","    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n","    \n","    def __init__(self, N, width_factor=2.0):\n","        self.N = N\n","        self.width_factor = width_factor\n","    \n","    @staticmethod\n","    def _gauss_basis(x, y, width, axis=None):\n","        arg = (x - y) / width\n","        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n","        \n","    def fit(self, X, y=None):\n","        # create N centers spread along the data range\n","        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n","        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n","        return self\n","        \n","    def transform(self, X):\n","        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n","                                 self.width_, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmpuNTnwiOTN","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(1)\n","x = 10 * rng.rand(50)\n","y = np.sin(x) + 0.1 * rng.randn(50)\n","xfit = np.linspace(0, 10, 1000)\n","\n","gauss_model = make_pipeline(GaussianFeatures(10, 1.0),LinearRegression())\n","gauss_model.fit(x[:, np.newaxis], y)\n","yfit = gauss_model.predict(xfit[:, np.newaxis])\n","\n","gf = gauss_model.named_steps['gaussianfeatures']\n","lm = gauss_model.named_steps['linearregression']\n","\n","fig, ax = plt.subplots()\n","\n","for i in range(10):\n","    selector = np.zeros(10)\n","    selector[i] = 1\n","    Xfit = gf.transform(xfit[:, None]) * selector\n","    yfit = lm.predict(Xfit)\n","    ax.fill_between(xfit, yfit.min(), yfit, color='gray', alpha=0.2)\n","\n","ax.scatter(x, y)\n","ax.plot(xfit, gauss_model.predict(xfit[:, np.newaxis]))\n","ax.set_xlim(0, 10)\n","ax.set_ylim(yfit.min(), 1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIm2gm1fJdto","colab_type":"code","colab":{}},"source":["x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"gZV1gHLJMn4v","colab_type":"text"},"source":["The shaded regions in the plot are the scaled basis functions, and when added together they reproduce the smooth curve through the data.\n","These Gaussian basis functions are not built into Scikit-Learn, but we can write a custom transformer that will create them, as shown here and illustrated in the following figure (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn's source is a good way to see how they can be created):"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"6jY_i0uwMn4w","colab_type":"code","colab":{}},"source":["gauss_model = make_pipeline(GaussianFeatures(20),LinearRegression())\n","gauss_model.fit(x[:, np.newaxis], y)\n","yfit = gauss_model.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, yfit)\n","plt.xlim(0, 10);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kmsYEoiTMn4x","colab_type":"text"},"source":["We put this example here just to make clear that there is nothing magic about polynomial basis functions: if you have some sort of intuition into the generating process of your data that makes you think one basis or another might be appropriate, you can use them as well."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"5e0sO7DeMn4y","colab_type":"text"},"source":["## 5.4 Regularization\n","\n","The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to over-fitting.\n","For example, if we choose too many Gaussian basis functions, we end up with results that don't look so good:"]},{"cell_type":"code","metadata":{"id":"wW2m3e1VJM5w","colab_type":"code","colab":{}},"source":["def PolynomialRegression(degree=2, **kwargs):\n","    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xG52TNPG8Cd_","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(1)\n","x = 10 * rng.rand(50)\n","y = np.sin(x) + 0.1 * rng.randn(50)\n","xfit = np.linspace(0, 10, 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"7u9A4py7Mn4z","colab_type":"code","colab":{}},"source":["model = make_pipeline(GaussianFeatures(30), LinearRegression())\n","model.fit(x[:, np.newaxis], y)\n","\n","plt.scatter(x, y)\n","plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n","\n","plt.xlim(0, 10)\n","plt.ylim(-1.5, 1.5);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Nw35O3WRMn40","colab_type":"text"},"source":["With the data projected to the 30-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data.\n","We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"P747ok7XMn41","colab_type":"code","colab":{}},"source":["def basis_plot(model, title=None):\n","    fig, ax = plt.subplots(2, sharex=True)\n","    model.fit(x[:, np.newaxis], y)\n","    ax[0].scatter(x, y)\n","    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n","    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n","    \n","    if title:\n","        ax[0].set_title(title)\n","\n","    ax[1].scatter(model.steps[0][1].centers_,\n","               model.steps[1][1].coef_)\n","    ax[1].set(xlabel='basis location',\n","              ylabel='coefficient',\n","              xlim=(0, 10))\n","    \n","model = make_pipeline(GaussianFeatures(30), LinearRegression())\n","basis_plot(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"UZy08MepMn43","colab_type":"text"},"source":["The lower panel of this figure shows the amplitude of the basis function at each location.\n","This is typical over-fitting behavior when basis functions overlap: the coefficients of adjacent basis functions blow up and cancel each other out.\n","We know that such behavior is problematic, and it would be nice if we could limit such spikes expliticly in the model by penalizing large values of the model parameters.\n","Such a penalty is known as *regularization*, and comes in several forms."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"g5qyTwZSMn43","colab_type":"text"},"source":["### 5.4.1 Ridge regression ($L_2$ Regularization)\n","\n","Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*.\n","This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n","$$\n","P = \\alpha\\sum_{n=1}^N \\theta_n^2\n","$$\n","where $\\alpha$ is a free parameter that controls the strength of the penalty.\n","This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"14u9vlq7Mn44","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import Ridge\n","model = make_pipeline(GaussianFeatures(30), Ridge(alpha=100))\n","basis_plot(model, title='Ridge Regression')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2kOS07FKMn47","colab_type":"text"},"source":["The $\\alpha$ parameter is essentially a knob controlling the complexity of the resulting model.\n","In the limit $\\alpha \\to 0$, we recover the standard linear regression result; in the limit $\\alpha \\to \\infty$, all model responses will be suppressed.\n","One advantage of ridge regression in particular is that it can be computed very efficiently—at hardly more computational cost than the original linear regression model."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"J9nIxsodMn47","colab_type":"text"},"source":["### 5.4.2 Lasso regression ($L_1$ regularization)\n","\n","Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n","$$\n","P = \\alpha\\sum_{n=1}^N |\\theta_n|\n","$$\n","Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.\n","\n","We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"IwAC_IioMn48","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import Lasso\n","model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.1))\n","basis_plot(model, title='Lasso Regression')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"awZ3zsKCMn5C","colab_type":"text"},"source":["With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions.\n","As with ridge regularization, the $\\alpha$ parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"wFxRPHqm8a6B","colab_type":"text"},"source":["## 5.5 Supervised learning example: Iris classification\n","\n","Let's take a look at another example of this process, using the Iris dataset we discussed earlier.\n","Our question will be this: given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n","\n","For this task, we will use an extremely simple generative model known as Gaussian naive Bayes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution.\n","Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before exploring whether improvements can be found through more sophisticated models.\n","\n","We would like to evaluate the model on data it has not seen before, and so we will split the data into a *training set* and a *testing set*.\n","This could be done by hand, but it is more convenient to use the ``train_test_split`` utility function:"]},{"cell_type":"code","metadata":{"id":"bCAD9laO8QZX","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","iris = sns.load_dataset('iris')\n","iris.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"4erCO_cx8a5Y","colab_type":"code","colab":{}},"source":["X_iris = iris.drop('species', axis=1)\n","X_iris.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"NtUhM22s8a5c","colab_type":"code","colab":{}},"source":["y_iris = iris['species']\n","y_iris.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"ZXCZfwj68a6B","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","# set the random state so you ge reproducible splits\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"nx8csmEj8a6D","colab_type":"text"},"source":["With the data arranged, we can follow our recipe to predict the labels:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"CIaMavce8a6E","colab_type":"code","colab":{}},"source":["# 1. choose model class\n","from sklearn.naive_bayes import GaussianNB\n","# 3. instantiate model\n","model = GaussianNB()      \n","# 4. fit model to data                 \n","model.fit(Xtrain, ytrain)       \n","# 5. predict on new data           \n","y_model = model.predict(Xtest) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4w_UB_Mvik1","colab_type":"code","colab":{}},"source":["y_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"wRM_EQF08a6F","colab_type":"text"},"source":["We can also use the ``partial_fit`` method multiple times if our data is too big and needs to be passed in chunks. \n","\n","Finally, we can use the ``accuracy_score`` utility to see the fraction of predicted labels that match their true value:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"E_nBX1hu8a6G","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(ytest, y_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Er0ZUWdh8a6I","colab_type":"text"},"source":["With an accuracy topping 97%, we see that even this very naive classification algorithm is effective for this particular dataset!"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"q4ixiWWj8a6I","colab_type":"text"},"source":["## 5.6 Unsupervised learning example: Iris dimensionality\n","\n","As an example of an unsupervised learning problem, let's take a look at reducing the dimensionality of the Iris data so as to more easily visualize it.\n","Recall that the Iris data is four dimensional: there are four features recorded for each sample.\n","\n","The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data.\n","Often dimensionality reduction is used as an aid to visualizing data: after all, it is much easier to plot data in two dimensions than in four dimensions or higher!\n","\n","Here we will use principal component analysis (PCA), which is a fast linear dimensionality reduction technique.\n","We will ask the model to return two components—that is, a two-dimensional representation of the data.\n","\n","Following the sequence of steps outlined earlier, we have:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"D85tpC9S8a6K","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA  # 1. Choose the model class\n","model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n","model.fit(X_iris)                      # 3. Fit to data. Notice y is not specified!\n","X_2D = model.transform(X_iris)         # 4. Transform the data to two dimensions\n","\n","iris['PCA1'] = X_2D[:, 0]\n","iris['PCA2'] = X_2D[:, 1]\n","\n","px.scatter(\n","    iris,\n","    x = \"PCA1\", \n","    y = \"PCA2\",\n","    color = 'species',\n","    width = 600,\n","    height = 400,\n","    template = 'plotly_dark',\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"MgaQ8yTR8a6M","colab_type":"text"},"source":["We see that in the two-dimensional representation, the species are fairly well separated, even though the PCA algorithm had no knowledge of the species labels!\n","This indicates to us that a relatively straightforward classification will probably be effective on the dataset, as we saw before."]}]}